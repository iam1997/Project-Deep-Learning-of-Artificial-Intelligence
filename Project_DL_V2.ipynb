{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5Y5RrptJ6yVLCFxV90JW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iam1997/Project-Deep-Learning-of-Artificial-Intelligence/blob/main/Project_DL_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MsmRBMxJTlc"
      },
      "source": [
        "# Import Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWlaOTQhcydO"
      },
      "source": [
        "### **Define all functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ_9KgYfinC-"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    W1 = np.random.randn(n_h, n_x)*0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h)*0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    \n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters     \n",
        "\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = W.dot(A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p\n",
        "\n",
        "def print_mislabeled_images(classes, X, y, p):\n",
        "    \"\"\"\n",
        "    Plots images where predictions and truth were different.\n",
        "    X -- dataset\n",
        "    y -- true labels\n",
        "    p -- predictions\n",
        "    \"\"\"\n",
        "    a = p + y\n",
        "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
        "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
        "    num_images = len(mislabeled_indices[0])\n",
        "    for i in range(num_images):\n",
        "        index = mislabeled_indices[1][i]\n",
        "        \n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n",
        "\n",
        "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []                              # to keep track of the cost\n",
        "    m = X.shape[1]                           # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "    \n",
        "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        A1, cache1 = linear_activation_forward(X, W1, b1, activation= \"relu\")\n",
        "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation= \"sigmoid\")\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        cost = compute_cost(A2, Y)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "        \n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n",
        "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (approx. 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "        \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "       \n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHKdP87sc_No"
      },
      "source": [
        "### **Import the MNIST-Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-yG_sSeloWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e2ba09-505b-40a2-e05d-f5bbc3165ec9"
      },
      "source": [
        "# Import the MNIST-Dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vos2GPdtd09o"
      },
      "source": [
        "# Convert the tuples to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDu08TgRloyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed77a14-ae61-4509-f4c6-299ce85c34ac"
      },
      "source": [
        "# Show the structure of the training dataset\n",
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('Number of training examples of the original dataset: ' + str(x_train.shape[0]))\n",
        "print('Structure of the training dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Show the structure of the test dataset\n",
        "(unique, counts) = np.unique(y_test, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('\\nNumber of test examples of the original dataset: ' + str(x_test.shape[0]))\n",
        "print('Structure of the test dataset')\n",
        "print(frequencies)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples of the original dataset: 60000\n",
            "Structure of the training dataset\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "\n",
            "Number of test examples of the original dataset: 10000\n",
            "Structure of the test dataset\n",
            "[[   0  980]\n",
            " [   1 1135]\n",
            " [   2 1032]\n",
            " [   3 1010]\n",
            " [   4  982]\n",
            " [   5  892]\n",
            " [   6  958]\n",
            " [   7 1028]\n",
            " [   8  974]\n",
            " [   9 1009]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_75zVP94XiAO"
      },
      "source": [
        "# !! The execution of this cell takes about 4 to 5 minutes\n",
        "# Remove images with labels 2 to 9 and the corresponding labels of the training dataset to have only a One-Class-Classification\n",
        "getIndexToRemove = []\n",
        "for getIndex in range(len(y_train)):\n",
        "  if y_train[getIndex] != 0 and y_train[getIndex] != 1:\n",
        "    getIndexToRemove.append(getIndex)     \n",
        "\n",
        "for removeIndex in reversed(getIndexToRemove):\n",
        "  x_train = np.delete(x_train, removeIndex, axis=0)\n",
        "  y_train = np.delete(y_train, removeIndex)\n",
        "\n",
        "# Remove images with labels 2 to 9 and the corresponding labels of the test dataset to have only a One-Class-Classification\n",
        "getIndexToRemove = []\n",
        "for getIndex in range(len(y_test)):\n",
        "  if y_test[getIndex] != 0 and y_test[getIndex] != 1:\n",
        "    getIndexToRemove.append(getIndex)     \n",
        "\n",
        "for removeIndex in reversed(getIndexToRemove):\n",
        "  x_test = np.delete(x_test, removeIndex, axis=0)\n",
        "  y_test = np.delete(y_test, removeIndex)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvC655IQXiRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28b1d0d-118a-4c31-aadb-e7a752354124"
      },
      "source": [
        "# Show the structure of the training dataset\n",
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('Number of training examples of the modified dataset: ' + str(x_train.shape[0]))\n",
        "print('Structure of the training dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Show the structure of the test dataset\n",
        "(unique, counts) = np.unique(y_test, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('\\nNumber of test examples of the modified dataset: ' + str(x_test.shape[0]))\n",
        "print('Structure of the test dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Show the image size\n",
        "print('\\nEach image has the size: (' + str(x_train.shape[1]) + ', ' + str(x_train.shape[2]) + ', 1)')\n",
        "print('--> Equal to ' + str(x_train.shape[1]*x_train.shape[2]*1) + ' pixels')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples of the modified dataset: 12665\n",
            "Structure of the training dataset\n",
            "[[   0 5923]\n",
            " [   1 6742]]\n",
            "\n",
            "Number of test examples of the modified dataset: 2115\n",
            "Structure of the test dataset\n",
            "[[   0  980]\n",
            " [   1 1135]]\n",
            "\n",
            "Each image has the size: (28, 28, 1)\n",
            "--> Equal to 784 pixels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtP5wST4m_It",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5f1bd308-f9eb-41b7-9c1f-bf4eeeb21521"
      },
      "source": [
        "# Show a random image of the test dataset\n",
        "plt.imshow(x_test[random.randint(0, x_test.shape[0]-1)]);"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxVkEnTLPRU3",
        "outputId": "ad9ffc11-53a0-43ee-c93c-b01bda183bc1"
      },
      "source": [
        "# Reshape the dataset \n",
        "x_train_flatten = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_test_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_train = y_train.reshape((1, y_train.shape[0]))\n",
        "y_test = y_test.reshape((1, y_test.shape[0]))\n",
        "\n",
        "# Show the shapes of the dataset\n",
        "print('Shape of x_train: ' + str(x_train_flatten.shape))\n",
        "print('Shape of x_test: ' + str(x_test_flatten.shape))\n",
        "print('Shape of y_train: ' + str(y_train.shape))\n",
        "print('Shape of y_test: ' + str(y_test.shape))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train: (784, 12665)\n",
            "Shape of x_test: (784, 2115)\n",
            "Shape of y_train: (1, 12665)\n",
            "Shape of y_test: (1, 2115)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kEvz7rFPRc5"
      },
      "source": [
        "# Standardize the data to have feature values between 0 and 1.\n",
        "x_train_standardize = x_train_flatten/255.\n",
        "x_test_standardize = x_test_flatten/255."
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tRgxMNnjwWn"
      },
      "source": [
        "### **L-Layer Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpx0HL9zUFDR"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        " \n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "       \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # Plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIcg_3-HPRfi"
      },
      "source": [
        "# Define constants for a 2-Layer Neural Network\n",
        "L1 = 1\n",
        "layers_dims = [x_train_flatten.shape[0], L1 ,1]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "1OmfsN9SPRiF",
        "outputId": "d3fc88eb-6e55-40e2-d3d9-5ff1c8f21833"
      },
      "source": [
        "# Train the Neural Network\n",
        "parameters = L_layer_model(x_train_standardize, y_train, layers_dims, num_iterations = 2500, print_cost = True)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.761552\n",
            "Cost after iteration 100: 0.379346\n",
            "Cost after iteration 200: 0.320286\n",
            "Cost after iteration 300: 0.282964\n",
            "Cost after iteration 400: 0.253853\n",
            "Cost after iteration 500: 0.229818\n",
            "Cost after iteration 600: 0.209517\n",
            "Cost after iteration 700: 0.192150\n",
            "Cost after iteration 800: 0.177150\n",
            "Cost after iteration 900: 0.164100\n",
            "Cost after iteration 1000: 0.152663\n",
            "Cost after iteration 1100: 0.142582\n",
            "Cost after iteration 1200: 0.133645\n",
            "Cost after iteration 1300: 0.125686\n",
            "Cost after iteration 1400: 0.118562\n",
            "Cost after iteration 1500: 0.112154\n",
            "Cost after iteration 1600: 0.106366\n",
            "Cost after iteration 1700: 0.101125\n",
            "Cost after iteration 1800: 0.096358\n",
            "Cost after iteration 1900: 0.092008\n",
            "Cost after iteration 2000: 0.088022\n",
            "Cost after iteration 2100: 0.084358\n",
            "Cost after iteration 2200: 0.080980\n",
            "Cost after iteration 2300: 0.077858\n",
            "Cost after iteration 2400: 0.074965\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Wa0AH2PRkr",
        "outputId": "21e3ad77-a7e1-4b76-e967-7bf65e18b324"
      },
      "source": [
        "# Make the prediction with the training dataset\n",
        "pred_train = predict(x_train_standardize, y_train, parameters)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9972364784840111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3B0p7bxPRnN",
        "outputId": "f051cc50-3489-4c47-828a-559ac3cb8fca"
      },
      "source": [
        "# Make the prediction with the training dataset\n",
        "pred_test = predict(x_test_standardize, y_test, parameters)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9990543735224583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8GKm0NSdl8U",
        "outputId": "caebd4d9-c37a-4136-8629-a29835f6c1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Lectura del set de datos Digitos\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "digits = load_digits()\n",
        "#Conocer cuantas imagenes cuenta y su dimension en el set de datos\n",
        "digits.data.shape\n",
        "#Obtener una muestra de la cantidad de imagenes de digitos\n",
        "n_digits = len(digits.images)\n",
        "#Split de train y test para generar entrenamiento supervisado para modelo de los digitos\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size= 0.2, random_state= 123)\n",
        "# Convert the tuples to numpy arrays\n",
        "x_train = np.array(X_train)\n",
        "x_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "plt.imshow(digits.images[7]);\n",
        "# Show the structure of the training dataset\n",
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('Number of training examples of the original dataset: ' + str(x_train.shape[0]))\n",
        "print('Structure of the training dataset')\n",
        "print(frequencies)\n",
        "# Show the structure of the test dataset\n",
        "(unique, counts) = np.unique(y_test, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('\\nNumber of test examples of the original dataset: ' + str(x_test.shape[0]))\n",
        "print('Structure of the test dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Remove images with labels 2 to 9 and the corresponding labels of the training dataset to have only a One-Class-Classification\n",
        "getIndexToRemove = []\n",
        "for getIndex in range(len(y_train)):\n",
        "  if y_train[getIndex] != 0 and y_train[getIndex] != 1:\n",
        "    getIndexToRemove.append(getIndex)     \n",
        "\n",
        "for removeIndex in reversed(getIndexToRemove):\n",
        "  x_train = np.delete(x_train, removeIndex, axis=0)\n",
        "  y_train = np.delete(y_train, removeIndex)\n",
        "\n",
        "# Remove images with labels 2 to 9 and the corresponding labels of the test dataset to have only a One-Class-Classification\n",
        "getIndexToRemove = []\n",
        "for getIndex in range(len(y_test)):\n",
        "  if y_test[getIndex] != 0 and y_test[getIndex] != 1:\n",
        "    getIndexToRemove.append(getIndex)     \n",
        "\n",
        "for removeIndex in reversed(getIndexToRemove):\n",
        "  x_test = np.delete(x_test, removeIndex, axis=0)\n",
        "  y_test = np.delete(y_test, removeIndex)\n",
        "\n",
        "# Show the structure of the training dataset\n",
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('Number of training examples of the modified dataset: ' + str(x_train.shape[0]))\n",
        "print('Structure of the training dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Show the structure of the test dataset\n",
        "(unique, counts) = np.unique(y_test, return_counts=True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print('\\nNumber of test examples of the modified dataset: ' + str(x_test.shape[0]))\n",
        "print('Structure of the test dataset')\n",
        "print(frequencies)\n",
        "\n",
        "# Show the image size\n",
        "print('\\nEach image has the size: (' + str(int(x_train.shape[1]/8)) + ', ' + str(int(x_train.shape[1]/8)) + ', 1)')\n",
        "print('--> Equal to ' + str(x_train.shape[1]*1) + ' pixels')\n",
        "\n",
        "# Reshape the dataset \n",
        "x_train_flatten = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_test_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_train = y_train.reshape((1, y_train.shape[0]))\n",
        "y_test = y_test.reshape((1, y_test.shape[0]))\n",
        "\n",
        "# Show the shapes of the dataset\n",
        "print('Shape of x_train: ' + str(x_train_flatten.shape))\n",
        "print('Shape of x_test: ' + str(x_test_flatten.shape))\n",
        "print('Shape of y_train: ' + str(y_train.shape))\n",
        "print('Shape of y_test: ' + str(y_test.shape))\n",
        "\n",
        "# Standardize the data to have feature values between 0 and 1.\n",
        "x_train_standardize = x_train_flatten/255.\n",
        "x_test_standardize = x_test_flatten/255.\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        " \n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "       \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # Plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "# Define constants for a 2-Layer Neural Network\n",
        "L1 = x_train_flatten.shape[0]\n",
        "layers_dims = [x_train_flatten.shape[0], L1 ,1]\n",
        "\n",
        "# Train the Neural Network\n",
        "parameters = L_layer_model(x_train_standardize, y_train, layers_dims, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# Make the prediction with the training dataset\n",
        "pred_train = predict(x_train_standardize, y_train, parameters)\n",
        "\n",
        "# Make the prediction with the training dataset\n",
        "pred_test = predict(x_test_standardize, y_test, parameters)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples of the original dataset: 1437\n",
            "Structure of the training dataset\n",
            "[[  0 139]\n",
            " [  1 148]\n",
            " [  2 141]\n",
            " [  3 150]\n",
            " [  4 139]\n",
            " [  5 145]\n",
            " [  6 138]\n",
            " [  7 148]\n",
            " [  8 137]\n",
            " [  9 152]]\n",
            "\n",
            "Number of test examples of the original dataset: 360\n",
            "Structure of the test dataset\n",
            "[[ 0 39]\n",
            " [ 1 34]\n",
            " [ 2 36]\n",
            " [ 3 33]\n",
            " [ 4 42]\n",
            " [ 5 37]\n",
            " [ 6 43]\n",
            " [ 7 31]\n",
            " [ 8 37]\n",
            " [ 9 28]]\n",
            "Number of training examples of the modified dataset: 287\n",
            "Structure of the training dataset\n",
            "[[  0 139]\n",
            " [  1 148]]\n",
            "\n",
            "Number of test examples of the modified dataset: 73\n",
            "Structure of the test dataset\n",
            "[[ 0 39]\n",
            " [ 1 34]]\n",
            "\n",
            "Each image has the size: (8, 8, 1)\n",
            "--> Equal to 64 pixels\n",
            "Shape of x_train: (64, 287)\n",
            "Shape of x_test: (64, 73)\n",
            "Shape of y_train: (1, 287)\n",
            "Shape of y_test: (1, 73)\n",
            "Cost after iteration 0: 0.692422\n",
            "Cost after iteration 100: 0.683744\n",
            "Cost after iteration 200: 0.679154\n",
            "Cost after iteration 300: 0.675255\n",
            "Cost after iteration 400: 0.671985\n",
            "Cost after iteration 500: 0.669707\n",
            "Cost after iteration 600: 0.667498\n",
            "Cost after iteration 700: 0.665262\n",
            "Cost after iteration 800: 0.662962\n",
            "Cost after iteration 900: 0.660577\n",
            "Cost after iteration 1000: 0.658109\n",
            "Cost after iteration 1100: 0.655537\n",
            "Cost after iteration 1200: 0.652852\n",
            "Cost after iteration 1300: 0.650047\n",
            "Cost after iteration 1400: 0.647117\n",
            "Cost after iteration 1500: 0.644043\n",
            "Cost after iteration 1600: 0.640808\n",
            "Cost after iteration 1700: 0.637404\n",
            "Cost after iteration 1800: 0.633840\n",
            "Cost after iteration 1900: 0.630135\n",
            "Cost after iteration 2000: 0.626266\n",
            "Cost after iteration 2100: 0.622234\n",
            "Cost after iteration 2200: 0.618013\n",
            "Cost after iteration 2300: 0.613415\n",
            "Cost after iteration 2400: 0.608645\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9930313588850175\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}